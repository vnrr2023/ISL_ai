{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bc967bb-332c-4ded-a6ed-8d4f35f09162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b4a41c2-0e5b-4f87-a5b0-8fc9140c6f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a732e067-a530-4251-b99b-806fd8038d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing=mp.solutions.drawing_utils\n",
    "mp_holistic=mp.solutions.holistic\n",
    "holistic=mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bf066ab-2e61-4552-b229-cd4465f8703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "style1=mp_drawing.DrawingSpec((131,255,0),1,1)\n",
    "style2=mp_drawing.DrawingSpec((255,221,0),1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c754256-b921-4df6-ada7-eaf5f4955c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_process(image,model):\n",
    "    image=cv.cvtColor(image,cv.COLOR_BGR2RGB)  # convert image from bgr to rgb\n",
    "    results=model.process(image)    # process the image or give to model\n",
    "    image=cv.cvtColor(image,cv.COLOR_RGB2BGR) # again convert rgb to bgr\n",
    "    return image,results   #return the image and the results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a82d36d-db49-476a-8fbf-2b410db382f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image,results):\n",
    "    mp_drawing.draw_landmarks(image,results.pose_landmarks,mp_holistic.POSE_CONNECTIONS,style1,style2)\n",
    "    mp_drawing.draw_landmarks(image,results.left_hand_landmarks,mp_holistic.HAND_CONNECTIONS,style1,style2)\n",
    "    mp_drawing.draw_landmarks(image,results.right_hand_landmarks,mp_holistic.HAND_CONNECTIONS,style1,style2)\n",
    "    mp_drawing.draw_landmarks(image,results.face_landmarks,mp_holistic.FACEMESH_CONTOURS,style1,style2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8aac924e-6217-4373-b2c9-ac37efdd21ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    _,frame=cap.read()\n",
    "    image,results=mediapipe_process(frame,holistic)\n",
    "    draw_landmarks(image,results)\n",
    "    if cv.waitKey(1)==27:\n",
    "        cap.release()\n",
    "        cv.destroyAllWindows()\n",
    "        break\n",
    "\n",
    "    cv.imshow(\"frame\",image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b30b0232-d6b4-46b0-9110-e5c8fd1cc9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x: 0.5417942\n",
       "y: 0.70010716\n",
       "z: -0.025448533"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.face_landmarks.landmark  #  we get list of landamrks\n",
    "results.face_landmarks.landmark[0]  # we get the first landmark object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1670144f-b26b-4da8-9402-45c73cf8f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "face=np.array([[landmark.x,landmark.y,landmark.z] for landmark in results.face_landmarks.landmark]).flatten()  if results.face_landmarks else np.zeros(1404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "740b3025-c90c-4f98-bf08-7311eec6835a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1404,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "23854859-ffe4-4340-98ee-963558a8e525",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose=np.array([[landmark.x,landmark.y,landmark.z , landmark.visibility] for landmark in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "714f6970-76c9-4f7a-b11d-a2ebe672ba2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((132,), 132)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose.shape,33*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c386a47-4237-4a54-bfbb-311bef368264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results.left_hand_landmarks.landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1d42d51-1e51-40c3-af4e-c00c7112cf19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x: 0.76958835\n",
       "y: 0.63348305\n",
       "z: 2.7013255e-07"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.left_hand_landmarks.landmark[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "198e94d4-30af-4a07-b8cd-31f203d4de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "lh=np.array([[landmark.x,landmark.y,landmark.z] for landmark in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "29fbce57-1b5a-4af0-8092-146ad0bbe293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.69588351e-01,  6.33483052e-01,  2.70132546e-07,  6.98390722e-01,\n",
       "        5.93119085e-01, -1.61111150e-02,  6.48746967e-01,  5.14213622e-01,\n",
       "       -2.08378024e-02,  6.31027758e-01,  4.34459209e-01, -2.63571087e-02,\n",
       "        6.28490269e-01,  3.70662779e-01, -3.13980430e-02,  6.86570883e-01,\n",
       "        4.08099413e-01,  2.96997512e-03,  6.62723601e-01,  3.21024448e-01,\n",
       "       -1.32737849e-02,  6.47231102e-01,  2.66576499e-01, -2.93239672e-02,\n",
       "        6.35680676e-01,  2.18851924e-01, -4.17023003e-02,  7.25880146e-01,\n",
       "        3.91054809e-01, -3.71778477e-03,  7.12408900e-01,  2.84705460e-01,\n",
       "       -1.78524610e-02,  7.01466084e-01,  2.17522681e-01, -3.35482731e-02,\n",
       "        6.92329586e-01,  1.61942691e-01, -4.48535495e-02,  7.65181482e-01,\n",
       "        3.94818842e-01, -1.48388082e-02,  7.54871905e-01,  2.95593262e-01,\n",
       "       -3.22471112e-02,  7.44168520e-01,  2.35549107e-01, -4.45502438e-02,\n",
       "        7.33477473e-01,  1.85654715e-01, -5.27232215e-02,  8.07887375e-01,\n",
       "        4.15152311e-01, -2.79874001e-02,  8.13515663e-01,  3.36923063e-01,\n",
       "       -4.36092727e-02,  8.12040985e-01,  2.86227584e-01, -4.98296469e-02,\n",
       "        8.07434857e-01,  2.40678594e-01, -5.37204668e-02])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cfd7ea12-705e-4342-8a9b-e3de58c66881",
   "metadata": {},
   "outputs": [],
   "source": [
    "rh=np.array([[landmark.x,landmark.y,landmark.z] for landmark in results.left_right_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "789ad1bd-7721-48e7-90ac-11ebad88b6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7600b5b-36fd-4261-8419-400577dfe168",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "1. we can see if left hand or the right hand is not detected then the results.left/right_hand will throw error\n",
    "   so we need to create a blank array same as the length of the total array\n",
    "   without flattening we get an array of shape = (21,3) 21: each landmark,3: x,y,z of each landmark\n",
    "   after flattening we get array of length 21*3=63 \n",
    "\n",
    "   // same for right hand\n",
    "   after flattening for right hand we get array of length 21*3=63\n",
    "\n",
    "   to avoid the error we create and array of all zeros of length 63  by using np.zeros(63)\n",
    "\n",
    "2. without flattening the pose landamrks array then shape is (33,4)\n",
    "   hence after flattening we get a single array of 33*4=132 lenght array\n",
    "\n",
    "   \n",
    "\n",
    "3. using face landmarks we can detect 468 landmarks on the face\n",
    "    before flattening shape of array is (468,3)\n",
    "    after flattening we get length=468*3=1404\n",
    "\n",
    "    create a blank array of all zeros\n",
    "\n",
    "4. np.concatenate([a,b,c]) will merge a,b,c arrays in one single array\n",
    "    after concatenating our arrays of pose lh rh and face we will get a single big array of length=468*3+33*4+2(21*3)=1662\n",
    "    total features\n",
    "\n",
    "5. our basic idea is that we will be collecting 30 frames per seconds which is 30fps and will be making predictions on those \n",
    "    30 frames values\n",
    "    for one class we will have n videos  ===== n actual data points\n",
    "        one video consists of 30 frames\n",
    "        each frame has 1662 values\n",
    "        total values we will collect to make 1 prediction will be 30*1662=49860\n",
    "\n",
    "        so our input shape to our model will be (30,1662)\n",
    "        oops thats too big isnt it?\n",
    "\n",
    "6. np.save(filename,array) will save the array as file with a npy extension\n",
    "    this is usefull to extract datapoints from each frame and store accordingly\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eb23ace5-f28c-4393-a62d-d6298ffce1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1662"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "468*3+33*4+2*(21*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6c3116ab-ba00-4e44-ae2f-c0cb3777af11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49860"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30*1662"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cf6c5f6e-af57-438d-8d17-978dddd99765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.arange(0,10)\n",
    "b=np.arange(10,20)\n",
    "\n",
    "np.concatenate([a,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a237f3aa-f455-4719-ba45-657b045ac9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmarks(results):\n",
    "    face=np.array([[landmark.x,landmark.y,landmark.z] for landmark in results.face_landmarks.landmark]).flatten()  if results.face_landmarks else np.zeros(1404)\n",
    "    pose=np.array([[landmark.x,landmark.y,landmark.z , landmark.visibility] for landmark in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "    rh=np.array([[landmark.x,landmark.y,landmark.z] for landmark in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(63)\n",
    "    lh=np.array([[landmark.x,landmark.y,landmark.z] for landmark in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(63)\n",
    "\n",
    "    return np.concatenate([pose,face,lh,rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5781f378-ac60-42b1-8935-9ac30cb97a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1662,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_landmarks(results).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ed8db90a-56ca-4b36-ae04-2808a09ccab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###################################\n",
    "        # Creating folders\n",
    "####################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a7726da-44fd-47af-91da-521f8afcf607",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10259d02-ec7f-44e8-ae64-4ea77f288b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions=np.array(['hello','thanks','iloveyou','please','blank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04ffb14d-1db6-4041-8a53-1e8a90de56f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_videos=30  #will be n for isl\n",
    "no_of_frames=30  # will be 30 only for isl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d7c598de-36e9-41e7-91c5-655c5463a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dirs\n",
    "# for action in actions:\n",
    "#     for i in range(no_of_videos):\n",
    "#         try:\n",
    "#             os.makedirs(os.path.join(data_dir,,str(i)))\n",
    "#         except:\n",
    "#             print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "eefb91a0-eb12-4c49-b247-a96c3dadba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('1',extract_landmarks(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b10230ab-6500-40bf-9f77-3eed054a1c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect data\n",
    "model=holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54ca4ae6-719d-4c17-81e7-e1a408849ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "thanks\n",
      "iloveyou\n",
      "please\n",
      "blank\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    cap=cv.VideoCapture(0)\n",
    "    for action in actions:  #  for each action\n",
    "        print(action)\n",
    "        for video in range(no_of_videos):  # for n videos\n",
    "            for frame in range(no_of_frames):\n",
    "                \n",
    "    \n",
    "                # read the frame from the camera/webcam\n",
    "                _,framee=cap.read()\n",
    "                image,results=mediapipe_process(framee,model)\n",
    "                draw_landmarks(image,results)\n",
    "                a=extract_landmarks(results)\n",
    "    \n",
    "                if frame==0:  # for the first frame\n",
    "                    cv.putText(image,f'starting collection',(100,200),cv.FONT_HERSHEY_COMPLEX_SMALL,3,(0,0,255),1,cv.LINE_AA)\n",
    "                    cv.putText(image,f'For {action}  Video {video} Frame {frame}',(120,32),cv.FONT_HERSHEY_COMPLEX_SMALL,1,(0,255,0),1,cv.LINE_AA)\n",
    "                    cv.imshow('image',image)\n",
    "                    cv.waitKey(4000)\n",
    "                else:\n",
    "                    cv.putText(image,f'For Video {action} {video} Frame {frame}',(120,32),cv.FONT_HERSHEY_COMPLEX_SMALL,1,(0,255,0),1,cv.LINE_AA)\n",
    "                    cv.imshow('image',image)\n",
    "    \n",
    "                a_path=os.path.join(data_dir,action,str(video),str(frame))\n",
    "                np.save(a_path,a)            \n",
    "                if cv.waitKey(5)==27:\n",
    "                    cap.release()\n",
    "                    break\n",
    "            \n",
    "    \n",
    "    cv.destroyAllWindows()\n",
    "    cap.release()\n",
    "except:\n",
    "    print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a278a511-c965-459d-a618-c895219209a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba06b440-9797-4347-895c-2ed88f9aaa92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
